\documentclass[10pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{COLAB: A Collaborative Multi-factor Scheduler for Asymmetric Multicore Processors
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

%\author{\IEEEauthorblockN{double-blind review\textsuperscript{} }
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address}
%\and
%}

\maketitle
%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
%\title{COLAB: A Collaborative Multi-factor Scheduler for Asymmetric Multicore Processors} 
%\author{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\input{abstract.tex}
\input{introduction.tex}
\input{background.tex}

\section{Multi-factor Coordinated Scheduler}
We analyze the performance impact of multiple runtime performance factors and their relationships with different functional units in the scheduler. We build a scheduler which addresses these performance problems in a coordinated way.

\begin{figure}
\centering
\includegraphics[scale=0.45]{figures/mfa.pdf}
\caption{A diagram of Performance Factors and Relationships with Scheduling Functions}
\label{figure:f1}
\end{figure} 

\subsection{Runtime Factor Analysis}
The relationships between runtime performance factors and the functions which address them is shown in Figure~\ref{figure:f1}. In order to achieve runtime collaboration, both core allocator and thread selector share information and account for all measured performance factors, including core sensitivity, bottleneck acceleration and fairness carefully as illustrated below:
%We describe them below by discussing each element. 
%\begin{itemize}
\subsubsection{Core Allocator}
AMP-aware Core allocators are mainly directed by the core sensitivity factor -- migrating a high speedup thread (with a large differential between big and little core execution time) from  a little core to execute on a big core will generally provide more benefit than migrating a low speedup thread. 

However, this heuristic is overly simplistic. Issues are revealed when the bottleneck factor is considered simultaneously on multiprogrammed workloads. Previous approaches \cite{jibaja2016portable} simply combine the calculation from  bottleneck acceleration and predicted speedup together, but this can result in suboptimal scheduling decisions -- both locking threads and high speedup threads may be accumulating in the runqueues of big cores as is illustrated in the motivating example in the Introduction. More intelligent core allocation decisions can be made by avoiding a simple combination of bottleneck acceleration and speedup -- the overall schedule can benefit from a more collaborative execution environment where big cores focus on high speedup bottleneck threads, and little cores handle other low speedup bottlenecked threads without additional migration.

%migrating threads with a lower predicted speedup, but which are blocking other threads.  
Furthermore, core allocators are desired to achieving relative fairness on AMPs by efficiently sharing heterogeneous hardware and avoiding idle resource as much as possible. 
Simply mapping ready threads uniformly between different type of cores can not achieve true load-balancing -- the number of ready threads prioritized on different type of core is different and thus, a hierarchical allocation should be applied to guarantee the overall fairness, which avoids the need to frequently migrate threads to empty runqueues. 

\subsubsection{Thread Selector}
Thread selector makes the final decisions on which thread will be executed during runtime. It is usually the responsibility of thread selector to avoid bottlenecking by thread blocking. In a multi-thread multiprogrammed environment, multiple bottleneck threads from different programs may need to be accelerated simultaneously with constraint hardware resources. Instead of simply detecting the bottleneck threads and throwing all of them to big cores as previous bottleneck acceleration schedulers \cite{jibaja2016portable,joao2013utility,joao2012bottleneck}, the thread selector needs to make collaborative decisions -- ideally, both big cores and little cores select bottlenecks to run simultaneously.

Core sensitivity is usually unrelated to the thread selector -- whether a thread can enjoy a high speedup from a big core compared with a little core is unrelated to which runqueue it is on, or came from. Therefore the thread selector should separate thread priority caused by core sensitivity and solely base decisions on bottleneck acceleration. One exception is that when the runqueue of a big core is empty and the thread selector is invoked -- the speedup factors from core sensitivity of ready threads should be considered only in this case. Big cores may even preempt the execution of little cores when necessary.  

The final concern of thread selector is about fairness. Scaling time slice of threads by updating the time interval of thread selector has been shown to efficiently guarantee the equal progress \cite{van2013fairness} in multi-threaded single-program workloads and achieve fairness. 
%In single-threaded multiprogrammed scenarios, complicated fairness formulation \cite{kim2018exploring} has been proposed to guide the thread selector for more precise decisions. 
Problems occur when targeting multi-threaded multi-programmed workloads. Simply keeping a thread-level equal progress is not enough to guarantee the multi-application level fairness -- the thread selector should ensure the whole workload is in relative equal progress without penalizing any individual application. In fact,  multi-bottleneck acceleration by both big and little cores does provide an opportunity for this - the thread selector makes the best attempt to keep fairness on all applications by accelerating bottlenecks from all of them and as soon as possible.


%\textbf{\textit{Fairness:}} Fairness is critical for system performance. A good schedule achieving relative fairness on AMPs should efficiently share heterogeneous hardware and avoid idle resource as much as possible. To achieve the expected relative equal-progress between threads and load-balancing between cores, the core allocator should map ready threads relative uniformly,avoiding the need to frequently migrate threads to empty runqueues. The design of the preemption triggering mechanism  also affects fairness, as it determines the maximum length of a time slice.


%The preemption triggering mechanism of thread selector is also related with the fairness factor as it determines the slice of a task running each time - a thread currently running on big cores should have relatively shorter time slice against its running on little cores to result in similar progress.
%The preemption should be triggered and the thread selector should be invoked less frequent on little cores than on big cores     

%\textbf{\textit{Core Sensitivity:}} Previous approaches considered core sensitivity and predicated speedup of threads but can result in poor scheduling decisions on multi-threaded, multi-programmed co-execution workloads. Selecting threads simply by predicted speedup on asymmetric cores may not lead to a overall good solution --  for instance, a high speedup thread detected on a little core could benefit by migrating to a big core, but is not blocking a significant number of other threads. The overall schedule can benefit more by migrating threads with a lower predicted speedup, but which are blocking other threads. Thus, we find priority from core sensitivity should be mainly addressed by the core allocator and be independent from the thread selector. 
%For instance, a thread with high predicted speedup should have more opportunities to be allocated and executed on a high-performance big core compared to a low predicted speedup thread. Higher speedup and more sensitive on big core does not indicate any {\it emergency} to execute this threads amount all other ready threads during the runtime. 
%No priority should be given to high-speedup threads for a thread selector, only if in a special case when a big core finishes its current task with an empty runqueue and all other big cores hold empty runqueue as well - so it needs to globally select a ready thread from a runqueue of little core and then relative higher speedup threads are better candidates. This special case won't even appear in large-scale parallel workloads scenarios where the number of co-executing programs is at least greater than the number of cores - there will always be waiting threads in a big core's runqueue based on fairness core allocation as no data-dependence between threads from different programs. 

%\textbf{\textit{Bottleneck Acceleration:}} Threads from multi-threaded programs holding contended locks and blocking other parallel threads should be identified as bottleneck and be executed as soon as possible. In large-scale workloads scenarios where multi-threaded multi-program are co-executing on limited AMPs resources and multiple blocking threads waiting to be executed simultaneously, the thread selector should take the main duty to select those bottlenecks and run the corresponding critical code segments in an efficient way without confused by the priority from core sensitivity. In detail, the thread selector triggered by big cores should always give higher priorities for a thread with higher blocking level against another thread with higher speedup level and lower bottleneck. It may preempt the running threads on little cores to accelerate them when suited. The thread selector triggered by little cores should also intend to select relative high blocking threads and accelerate them locally first, instead of trivially migrating it to big cores and waiting to be actual accelerated there.  

\begin{figure}
\centering
\includegraphics[scale=0.57]{figures/COLAB_M.pdf}
\caption{Coordinated Model by Multi-factor Collaboration}
\label{figure:f2}
\end{figure} 

\subsection{Collaboration}
To address the problems detailed above, we designed a coordinated multi-factor scheduler in which the core allocator and the thread selector collaborate to achieve high performance and high fairness, when compared to the state-of-the-art mixed multi-factor evaluator in WASH \cite{jibaja2016portable}.
%While the way we define whether a thread's relative speedup value is high or low is similar as in WASH based on hardware configuration - if we have same number of big and little cores, then we rank all ready threads based on its relative speedup and view the first half as high speedup value. 
The flowchart of our model is shown in Figure~\ref{figure:f2}. 
Collaboration is facilitated by periodically labeling ready threads in two different categories, based on runtime models of speedup prediction and bottleneck identification: 
%\begin{itemize}
\subsubsection{Labels for Core Allocation}
Threads with high predicted speedup between big and little cores will be labeled as high priority on big cores. Threads with both low predicted speedup and blocking levels -- non-critical threads -- will obtain high priority on little cores (and low priority on big cores). Remaining threads obtain equal priority on either big or little cores -- these threads can then be allocated freely to balance the load of cores.
\subsubsection{Labels for Thread Selection}
Threads with high blocking level will be labeled as high priority on local thread selection. The same priority will be given on these blocking threads whether the issuing cores are big or little, so the labels of thread selection do not distinguish the type of cores.The label nevertheless records the type of the current core -- threads always have priority to be selected by the same type of cores if there exists a core of the same type with an empty runqueue. Running threads on little cores are also labeled as they may be preempted to migrate and execute on big cores when suited, but running threads will never have priority over waiting ready threads. 


%The thread labels should be not only be based on the relative ranking between threads, but also on boundary conditions targeting different hardware resource and experimental environments. 
%We setup empirical boundaries for speedup and blocking: (1) No thread is ranked as high-speedup if its predicted speedup value is less than an empirical architecture-specific boundary, calculated using relative frequency between big and little cores. 

%For instance, if the big cores' frequency is 2x faster than the little cores and ready threads get around 1.5x predicted speedup, this likely means threads haven't be executed long enough to get a more precise speedup value and we should not bind or migrate any thread at this step. 
%(2) Threads will not be ranked as high-blocking if the rate of their blocking time and life time are less than a relative boundary or the life time of them are less than a starting point - For instance, no thread should be ranked as relative high-blocking during the initial time periods of experiments.   
%It is mainly benefit in a mixed workloads scenario where we co-execute single and multi-threaded multiprogrammed - a non-computing-intensive code segment from a single thread program can neither enjoy good performance gain on big cores nor block others. So we should better keep it in a little core and let other blocking threads to be executed before it. 

After the labeling process, fairness, core sensitivity and bottleneck acceleration are represented by labels on threads can be handled by either the core allocator or the thread selector or both together. Based on this coordinated model, the core allocator and thread selector handle different priorities queues from the set of ready threads -- their decisions are not greedy on a mixed multi-factor ranking like WASH, rather provide a collaborative schedule.

Another important issue handled by the collaborative multi-factor model is to ensure relative equal-progress of threads as shown in the upper-right corner of Figure~\ref{figure:f2}. Instead of interfering with the priority and decisions of thread selection, we achieve equal progress in threads by our scaled time slice approach, based on the predicted speedup value of threads running on big cores. The slices of threads on big cores are relative shorter than on little cores. The thread selection function is triggered more often to swap executing threads on big cores, which guarantees the relative equal-progress of threads executed on all cores.

The runtime model periodically extracts the performance counters, which represents the current execution environment of multi-threaded multi-programmed workloads on the AMPs. The model then computes the updated runtime factors, including the predicated speedup value and blocking counts. This information is attached to the threads and reported back to the multi-factor labeler for next round. We present our runtime model implementations in the section below. 

\section{Runtime Design and Implementation}
We implement our approach on the GEM5 simulator \cite{binkert2011gem5}, modifying the simulator and constructing interfaces between the Linux kernel v3.16 with the CFS scheduler.

\subsection{Runtime Factors Implementations}
To implement the runtime multi-factor model, we update the main scheduler function \texttt{\_\_sched\_\_schedule()} of the Linux kernel by adding a thread labeling process as described in section 3.2 above. The similar method is applied when we re-implement the WASH scheduler by updating thread affinities without the help of VM. 

We present our implementation of the performance analyses we use to achieve optimization:

\textbf{\textit{Machine Learning based Speedup Prediction:}} Predicting relative speedup of each thread on heterogeneous cores is a core functionality of any core sensitivity aware scheduler targeting AMPs. Our implementation of the speedup model follows the common approach in the literature \cite{van2013fairness,jibaja2016portable,saez2012leveraging}- an off-line trained predictive model applying on on-line. 

To construct the training set, we run all applications in single-program mode with two basic SMP configurations - full little cores and full big cores. We compute the speedup of each application between the heterogeneous cores and record all 225 performance counters from the simulated ARM cores in detailed CPU model from GEM5 \cite{binkert2011gem5} in full big core execution. The Principal Component Attribute Transformer (PCAT) technique \cite{witten2016data} is applied to select six performance counters for use as features from the initial larger set. We choose six performance counters, plus a counter of committed instructions (used for normalization) as this is a typical number implemented in real hardware, such as Intel Sandy Bridge \cite{jibaja2016portable}. Finally, linear regression is applied on the applications set with selected counters and actual speedup to train the weight value of each counter. The selected counters and predictive linear speedup model targeting ARM big.LITTLE processors running on GEM5 are shown in table \ref{pca_sp}.  

\begin{table*}
  \caption{Selected performance counters and Speedup Model}
  \center
  \label{pca_sp}
   \scalebox{1}{
   \begin{tabular}{p{2cm} | p{6cm} | p {9cm} c c c}
 % \begin{tabular}{l c c c c}
  \hline
    \multicolumn{3}{c}{Selected GEM5 performance counters by PCAT}\\
  \hline
     Index&  Name& Description \cite{binkert2011gem5} \\
    \hline
     A: & fp\_regfile\_writes & number of integer regfile writes\\
     B: & fetch.Branches & number of branches that fetch encountered\\
     C: & rename.SQFullEvents & number of times rename has blocked due to SQ full\\
     D: & quiesceCycles & number of cycles  quiesced or waiting for an interrupt\\
     E: & dcache.tags.tagsinuse & cycle average of tags of dcache in use\\
     F: & fetch.IcacheWaitRetryStallCycles & number of stall cycles due to full MSHR\\
     \hline
     G: & commit.committedInsts & number of instructions committed\\
     \hline
    % \toprule[1pt]
     \multicolumn{3}{c}{Linear predictive speedup model}\\
     \hline
     \multicolumn{3}{c}{2.6109+((0.0018*-0.185A)+(0.0259*0.187B)+(0.1047*0.194C)+(-0.023*0.238D)+(0.0492*-0.299E)+(-0.1388*-0.227F))/G}\\
  %  \bottomrule
  \hline
  \end{tabular}}
\end{table*}



\textbf{\textit{Bottleneck Identification:}}
On modern Linux systems synchronization primitives are almost always implemented using kernel futexes, regardless of the threading library used. Futex-based mechanisms use a single atomic instruction in user space to acquire or release the futex, if it is uncontested. Otherwise, it triggers a system call which forces the thread to sleep or wakes up sleeping threads, respectively.

This gives us a convenient single point where we can monitor blocking patterns between threads. We first add code in \texttt{futex\_wait\_queue\_me()} and \texttt{futex\_lock\_pi()}, right before the active thread starts waiting on a futex. We record the current time and store it in the \texttt{task\_struct} of the thread. We then insert code in \texttt{wake\_futex()} and \texttt{wake\_futex\_pi()}, right before the waiting task is woken up by the thread releasing the futex. There we calculate the length of the waiting period and we accumulate it in the \texttt{task\_struct} of the thread releasing the futex. This way we are able to measure the cumulative time each thread has caused other threads to wait. We use this as our metric of thread criticality for the rest of the paper.

\textbf{\textit {Speedup based Scale-slice Preemption:}} Although we implement our scheduler on Linux kernel by fully re-writing both the core allocator and thread selector, the underlining preemption mechanism of Linux is applying the virtual runtime {\it vruntime} in CFS with red-black tree data structure - whenever a new task is enqueued, a preemption wake-up function is invoked to check whether the new coming task should preempt the current task by computing the difference in vruntime and comparing with a boundary. 

To achieve relative equal-progress on AMPs, threads running on different types of cores should have different time slices instead of trying to achieve complete fairness on time. We update the default preemption wake-up function \texttt{wakeup\_ preempt \_entity()} in Linux by constructing an interface to the GEM5 simulator. We apply our runtime speedup model to update the vruntime of the current task by dividing it by the thread's speedup value if the triggering core is a big core. The ensures relative equal progress.




\subsection{Scheduling Algorithm Design and Implementation}
\begin{algorithm}
\caption{Collaborative Multi-factor Scheduler targeting Asymmetric Multicore Processors}
\label{alg:1}
\begin{algorithmic}[1]
\STATE core\_alloctor\_(thread\_struct\ $t$)\{
%\FOR {$t : ready\_threads$}
%\IF {t->high\_speedup() \& t->high\_block()}
\IF {t.high\_speedup}
\RETURN rr\_allocator\_(big\_cores)
\ENDIF
\IF {t.low\_speedup \& t.low\_block}
%\& !unique(t->tgid)}
\RETURN rr\_allocator\_(little\_cores)
\ENDIF
\RETURN rr\_allocator\_(cores)
\STATE \}
\STATE thread\_selector\_(core\_struct\ $c$)\{
%\IF {c->cur \& !(preempt_wakeup)}
\IF {!empty(c.rq)}
\RETURN max\_block\_(c.rq)
\ENDIF
%\IF {c->type == big}
\IF {!empty(c.sched\_domain.rq)}
\RETURN max\_block\_(c.sched\_domain.rq)
\ENDIF
\IF {c.cpu\_mask == big}
%\FOR {t : little\_core->cur}
%\STATE candidates.enqueue(t)
%\IF {!empty(candidates)}
\RETURN  max\_block\_(c.sched\_domain\_little.cur)
\ENDIF
%\ENDFOR
%\ENDIF
%\FOR {t : other\_little\_core->rq}
%\STATE candidates.enqueue(sorted\_speedup(t))
%\ENDFOR
%\IF {!empty(candidates)}
%\RETURN  max\_block\_(candidates)
%\ENDIF
%\ENDIF
%\FOR {t : other\_core->rq}
%\STATE candidates.enqueue(reversed\_speedup(t))
%\ENDFOR
\RETURN idle
\STATE \}
\end{algorithmic}
\end{algorithm}

Our scheduling algorithm is implemented by fully overriding the default task selector \texttt{pick\_next\_task\_fair()} and core allocator \texttt{select\_task\_rq\_fair()} in Linux kernel supported by the runtime factors. The pseudo-code of our scheduling algorithm is shown in Alg. \ref{alg:1}. 
Similarly to commonly used Linux notations, we use $rq$ to represent runqueue and $cur$ to represent the current task of a core in our code. We describe the two main functions below:

\textbf{\textit{Hierarchical Core Allocator:}}
When a thread is ready to be executed, whether it was just spawned for woken, the core allocator will be invoked to allocate this thread to a possible core's runqueue. To achieve relative load balancing and consider the influence from core sensitivity factor, we involve a hierarchical round-robin mechanism \texttt{rr\_allocator\_()}. Indicated by the speedup and blocking labels, threads are allocated to different core groups. Threads with high speedup level will be round-robin allocated in big core clusters (line 3). Low speedup and low blocking thread will only be allocated in round-robin over the little core clusters (line 5). 
%A technical issue here is we need to distinguish threads which are part of a multi-thread program from from single-thread programs - these single-thread program threads will definitely not block others and may not have a high speedup but we should nevertheless not bind them to little cores. They can be easily detected by the allocator by checking whether there are ready threads whose group pid (t->tgid) equals to any others (line 5). 
To fill the gap of different core clusters, remain ready threads (usually with normal speedup level and tiny block) will be relatively equally allocated to both core types by \texttt{rr\_allocator\_(core)} - this final round-robin shares counters with the first two big/little core specific round-robin to achieve overall load balancing (line 6).  

\textbf{\textit{Biased-global Thread Selector:}}
The thread selector is based on the principle of accelerating the most critical/blocking thread as soon as possible, shown in line 10, 12, 14. Triggered by different cores, the selector always try to return  a local candidate first instead of invoking additional migrations (line 10). When there are no local waiting threads and migration is beneficial, cores give priority for candidate threads waiting on other's runqueue and from there the most blocking thread will be selected.
To reduce the overhead of accessing global CPUs states, we follow the same principle of the default Linux CFS scheduling domain (\texttt{struct sched\_domain}) - returning the best candidate thread from the local core group first (line 12).
Further, we allow a big core to select and preempt a running thread on little core to accelerate it (line 13,14). Idle resources will only occur when there is not any possible useful work to be done (line 15) - for instance, we do not allow a little core to preempt a big core's execution. 
In summary, the thread selector can still access all other global runqueues when necessary, but it is biased to access nearly runqueues first, due to bottleneck and core sensitivity concerns. 
Note that the relative equal-progress for achieving fairness is addressed by the scale-slice preemption checker instead of the thread selector -- we give each thread a maximum time slice relative to its expected performance on the asymmetric core.

\textbf{\textit{Discussion:}}
This section provides a case study, implementing our scheduling framework targeting simulated ARM architectures and processors on GEM5.  However, the underlining general procedure and model can be implemented on real processors by using their hardware performance monitor events (PMU). All hardware counters used by our model are supported by the ARM Cortex-A57/A53 \cite{ARMA57} PMU.
%For instance, we input 150 PMU events \cite{ARMA57} of ARM Cortex-A57/A53 to the PCA process to build up corresponding speedup model when implement on real ARM cores.
%To further minimize the overhead of accessing global CPUs states to implement our approach on real systems, the thread selector can be configured to check other cpus one by one instead of 

\input{evaluation.tex}
%\vspace{10mm}
\section{Conclusion}
This paper presents a novel scheduling framework targeting multi-threaded multiprogrammed workloads on asymmetric multicore processors. This scheduler is the first to account for all three factors affecting AMP scheduling: core affinity, thread criticality, and scheduling fairness, using a general purpose asymmetry-aware scheduler.

Over a representative selection of multi-programmed workloads, 
%we achieve more than 10\% performance gain compared to Linux, and more than 5\% performance gain to a state-of-the-art heterogeneous-aware scheduler on average.
Our COLAB improves performance by up to 25\% and 21\% lower trunaround time, 11\% and 5\% on average with similar advantage on throughput, compared to Linux and a state-of-the-art heterogeneity-aware scheduler.

%27\% on average on different hardware setups when compared to the Linux CFS scheduler and

%Compared with the default Linux CFS scheduler and the state-of-the-art AMP-aware WASH scheduler, COLAB achieves up-to 51\% performance gain in heterogeneous average normalized turnaround time (H\_ANTT) while 5\% to 27\% on average depending on the hardware setup and up-to 30\% increase on heterogeneous system throughput (H\_STP) in geomean targeting multi-threaded multiprogrammed co-executed workloads. 
%It performs significantly better when workloads contain both multi-threaded and single-thread applications achieving performance gain up to 54\% in average normalized turnaround time and 46\% increase on system throughput against both the previous approaches.

%COLAB aims to highlight the potential of further performance gain for AMP-aware scheduler targeting multi-threaded multiprogrammed co-execution and encourage following research by applying coordinated multi-factor runtime frameworks with core allocator and thread selector collaboration. 

%% Acknowledgments
%\section*{acks}
% List grants

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ieeetr}
\bibliography{references}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
