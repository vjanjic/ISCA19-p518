\section{Background and Related Work}
\label{rw}

\begin{table}[htbp]
  \caption{Qualitative Analysis on Related Work}
  \begin{center}
  \label{rwt}
  \scalebox{0.9}{
   \begin{tabular}{p{3cm} p{1cm} p{1cm} p{1cm} p{2cm} c c c c c }
   \hline
     %\toprule[1pt]
     Approaches&Core Sens.&Fairness&Bottle- neck&Collaborative\\
     %\toprule[1pt]
     \hline
    Kumar, et al \cite{kumar2004single} &$\checkmark$&&&\\
    Li, et al \cite{li2009efficient}&&$\checkmark$&&\\
    Suleman, et al. \cite{suleman2009accelerating}&&&$\checkmark$&\\
    Saez, et al. \cite{saez2012leveraging}&$\checkmark$&$\checkmark$&&\\
    Craeynest, et al. \cite{van2013fairness}&$\checkmark$&$\checkmark$&&\\
     Cao, et al. \cite{cao2012yin}&$\checkmark$&&&\\
     Joao, et al \cite{joao2013utility}&$\checkmark$&&$\checkmark$&\\
     Kim, et al \cite{kim2018exploring}&$\checkmark$&$\checkmark$&\\
     Jibaja, et al \cite{jibaja2016portable}&$\checkmark$&$\checkmark$&$\checkmark$\\
    \textbf{COLAB}&$\checkmark$&$\checkmark$&$\checkmark$&$\checkmark$\\
    \hline    
%\bottomrule
  \end{tabular}}
  \end{center}
\end{table}

Initially described by Kumar et al.~\cite{kumar2004single,kumar2003single}, single-ISA heterogeneous multicore processors allow for more efficient processing, but to realize this we need the OS scheduler to match threads with cores more suited to their requirements. A straightforward way to determine good matches is based on the \emph{IPC} of the application on each kind of core. While easy to understand and perform, IPC is a reliable metric of performance only for single threaded applications, the search might be lengthy, and it will be affected by resource sharing and phase changes.

To work around some of these problems, other approaches have used performance models to predict the speedup due to executing a thread on another core type. Saez et al.~\cite{saez2012leveraging} build such a model based on \emph{ILP} and \emph{LLC miss rates}, Craeynest et al.~\cite{van2012scheduling} used \emph{CPI stack}, \emph{ILP}, and \emph{MLP}, while Jibaja et al~\cite{jibaja2016portable} applied Principal Component Analysis to select the performance counters most closely correlated with performance and built a linear model out of them. In all cases, the predicted speedup is used to decide \emph{Core Sensitivity}, how sensitive the thread's performance is on the type of core used. The more sensitive threads are assigned to high performance cores exclusively, the rest can be assigned to any type of core.

%\emph{Core sensitivity} has been a main concern since they come up with the AMPs - they executed threads on each type of cores and then used {\it IPC} to guide the selection. To build up a more precise model, more performance counters were considered to predict the relative speedup, such as $ILP$ and {\it LLC miss rates} applied by Saez, et al. \cite{saez2012leveraging} in their speedup factor estimation model and {\it CPI stack, ILP and MLP} by Craeynest, et al \cite{van2012scheduling} in their performance impact estimation (PIE). Addition helps from VM services have also been discussed and involved, as presented by Cao, et al. \cite{cao2012yin}. It provided a new opportunity of high-efficiency for AMPs scheduling by binding low priority tasks, such as VM helper threads, to the little cores. Furthermore, machine learning based approaches with the help of VM has been applied to further select performance counters and predict the speedup. As illustrated by Jibaja, et al. in \cite{jibaja2016portable}, principal component analysis is applied to select important performance counters from the initial comprehensive set and then regression is conducted to finalize the prediction model.

\emph{Acceleration of bottleneck and critical threads} is also necessary for high performance on AMPs. Kumar et al.~\cite{Kumar:2005:HCM:1100859.1100890} early on identified the benefit of executing Amdahl's serial bottlenecks on high performance cores, while executing parallel code in low performance, low power cores. Suleman et al.~\cite{suleman2009accelerating} proposed accelerating critical code sections too, in order to minimize the time a thread works on shared data and keep such data on the big core caches. Joao et al.~\cite{joao2012bottleneck,joao2013utility} generalized this idea by identifying and accelerating bottleneck functions dynamically. Using programmer hints and hardware support, they measure the number of cycles spent by each thread waiting on data from a (potentially) bottleneck function. If above a waiting cycle threshold, the function is accelerated. Jibaja et al.~\cite{jibaja2016portable} proposed finding bottleneck Java threads by measuring waiting time on contended locks.

Maintaining \emph{scheduling fairness} is an additional challenge introduced by AMPs. Fair schedulers try to balance the processing time given to each thread, process, or process group. Most implementations regard all sources of processing time as equivalent, which is not the case with AMPs. Li et al~\cite{li2007efficient} introduced asymmetry-aware load balancing where the load assigned to each core is proportional to its processing power. Craeynest et al.~\cite{van2013fairness} built an \emph{equal progress} scheduler. Using their performance model they were able to estimate the amount of small core processing time that each core should be given to progress as much as it has. The scheduler then prioritized threads so that the progress of all threads is the same. Multiple other scheduling heuristics have tried to maximize fairness on AMPs~\cite{zahedi2018amdahl,wang2016rebudget,kim2018exploring} but for restricted scheduling scenarios.

%In addition to the core sensitivity and speedup factor, AMPs also brought new challenges to keeping fairness during execution. The default Linux Completely Fair Scheduler (CFS) from kernel version v2.6.23 developed by Ingo Molnar \cite{molnar2007cfs} achieved complete fairness for multiprogrammed execution on a CPU. The underlining red-black tree structure and visual runtime {\it vruntime} based preemption mechanism kept multiprogrammed running in fairness progress without a timeslice setup or any heuristic needs. To deal with the challenge from AMPs, Li et al \cite{li2007efficient} designed framework to first schedule threads on high-performance cores and then keep  asymmetry-aware load balancing between AMPs. Followed by they involved distributed weight round-robin \cite{li2009efficient} to improve efficiency further and achieve high-scalable in addition of fairness on multicore processors. But the weight value in their approaches was only based on a default static priority of threads and no concern on the core sensitivity issue. Craeynest, et al. \cite{van2013fairness} proposed a AMP-oriented fairness model with a core sensitivity concern. Equipped with their previous PIE model to predict relative speedup as described above, they designed an $equal$-$progress$ fairness approach to keep multi-threaded programs running in relative equal progress on AMPs by updating their actual execution time based on predicted big-versus-small-core scaling factor from PIE. Jibaja, et al \cite{jibaja2016portable} achieved its equal progress by dynamic workload analysis and classification and then left the final scheduling decisions by CFS.
%More complex fairness metrics and model have been proposed recently, including the market-based models either by Zahedi, et al. \cite{zahedi2018amdahl} or Wang and Martinez \cite{wang2016rebudget} and the uniformity fairness scheduling by Kim and Huh \cite{kim2018exploring}. While those approaches were either only focusing on processor allocation instead of the runtime scheduling or targeting single-thread multiprogrammed  to build up their representation, for which the total amount of jobs for each thread is deterministic. So achieving a relatively equal progress is still the main concern for fairness on multi-threaded scheduling on AMPs. 

Among all previous work on AMP schedulers, only Kim and Huh~\cite{kim2018exploring} and Jibaja et al.~\cite{jibaja2016portable} targeted the general case of multi-threaded multi-programmed workloads. The uniformity fairness policy~\cite{kim2018exploring} focuses only on fairness and core sensitivity, without provision for bottleneck acceleration. WASH~\cite{jibaja2016portable} is the closest existing scheduler to ours. It handles core sensitivity, bottlenecks, and maintains fairness for the general scheduling case but controls only core affinity, leaving all other decisions to the baseline Linux scheduler. We use a WASH-like implementation for the Linux scheduler  as our state-of-the-art. A summary with qualitative comparison on the related work is shown in Table \ref{rwt}.

%The fact that Linux CFS results better than an AMP-aware scheduler like WASH in certain workloads is counter-intuitive. One reason is that some embarrassed parallel workloads do not need much synchronization/multi-threaded communication. So the blocking counter always output all-zero which is useless for decisions but increasing runtime overhead. Another reason is no much actual speedup for threads in some symmetric multi-threaded workloads when each thread are doing similar job with equally partitioned data. Speedup model is then inaccuracy and leads to bad heuristics with useless migrations

%Other recent AMP-aware schedulers either focusing on special targets, such as high-reliability by Naithani, et al. \cite{naithani2017reliability} and tail latency by Haque, et al. \cite{haque2017exploiting}, or on more complex AMPs with heterogeneous-ISA by Venkat, et al. \cite{venkat2014harnessing} which are beyond the scope of our general-propose work here. 
